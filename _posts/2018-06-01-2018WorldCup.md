---
layout: post
title: "Probabilistic football model for the world cup"
description: "A ton of text to test readability."
tags: [data science, football, Variational Inference, Pyro]
categories: [data science]
jsarr:
- graphs/world_cup.js
- graphs/world_cup_simulation.js
---

Probabilistic programming is an exciting area of data science, but one in which there are still many different competing frameworks (even only within the python ecosystem). I've been meaning to play around with one of the newer frameworks from Uber, and based on pytorch: Pyro
Since the football world cup is just around the corner, I thought it would be a good opportunity to use Pyro to not only try to predict the result of any of the matches, but also do something thta probabilistic programming is particularly useful for: inferring hidden variables.

Pyro's framework seems especially designed for inference through variational inference. Inference with Pyro requires mainly two things:
- A generative model, specified through a function
- A specification of the functional form of the posterior, which they call a `guide`

This is quite a simple and elegant way of specifying how to run inference on what can be fairly complex models. 

<!-- more -->

## Simple model

Imagine for a second that you've never actually seen a football game, you have no idea how it is played and the only thing you know about it is that it is played (usually) over 90 minutes, involves two teams and scores typically range from 0 to 5. The simplest hypothesis you'd probably come up with is that each team has a given rate of goal scoring (e.g. team A scores 2.3 goals per game on average), and as a first approximation you can assume that the rate is constant and doesn't depend on the other team, the weather, the tournament etc. This is a very simple Poisson model, in which the only latent variable is the team's scoring rate! Here's an example of how to specify it in Pyro

{% highlight python %}

# data specification
data = [
	{
	"team A": 0,
	"team B": 1
},
]

def score_model(data):
    # separate prior over scoring rate for every team
    rate_dict = {team: pyro.sample("scoring_rate_{}".format(team), 
    			dist.Gamma(conc_hyperprior, 1.0 / inv_rate_hyperprior))
                for team in teams}

    # now we can generate data for all the games in our dataset
    # the `obs=` kwarg signifies that we're also going to calculate
    # the likelihood under this model for a later optimisation
    for i, game in enumerate(data):
        for j, team in enumerate(sorted(game.keys())):
            score = torch.tensor(float(game[team]))
            rate = rate_dict[team]
            score_sample = dist.Poisson(rate)
            pyro.sample("score_game{}_{}".format(i, team), score_sample, obs=score)

# here we condition the generative model on the measurements
conditioned_score_model = pyro.condition(score_model, data=data)

{% endhighlight %}

What Pyro does under the hood (when inference is run) is take this generative model and calculate the likelihood of the data under the model. 
Pyro has a parameter store, which will keep track of the likelihoods of the different measurements (score_game* here) for different values of the latent variables. We have also specified here what our priors are for our latent variable, where we simply take Gamma distributions for each team. So far this is a nice pythonic way of specifying the generative model, but we are still missing the functional form of the posterior. In this simple model, it will mirror the generative model nicely. We have to remember that variational inference is an optimization over some scalar parameters so the simplest way to do this is through optimizing over gamma parameters for each team separately. Pyro will look for any `pyro.param` that is specified in the guide and will run an optimization over these parameters to maximize the evidence lower bound (ELBO). In this guide you need to specify how these `pyro.param` influence parameters within the generative model. In our case, we will optimize over the concentrations and inverse rates of a set of gamma posteriors. 

{% highlight python %}

def score_parametrized_guide(data):
    # we can start the optimization at the hyperprior, specified in
    # the `score_model`
    conc = {team: pyro.param("concentration_{}".format(team), 
                             conc_hyperprior)
           for team in teams}
    inv_rate = {team: pyro.param("inv_rate_{}".format(team), 
                             	 inv_rate_hyperprior)
           for team in teams}
    # This mirrors the `scoring_rate_` parameters in the generative model
    rate_dict = {team: pyro.sample("scoring_rate_{}".format(team), dist.Gamma(conc[team], 1.0 / inv_rate[team]))
                 for team in teams}
    return rate_dict

{% endhighlight %}  

To run the inference, we just specify what kind of inference we want (stochastic variational inference in our case), the loss we're interested in, and the optimization algorithm

{% highlight python %}

# remember to clear any lingering params
pyro.clear_param_store()
svi = pyro.infer.SVI(model=conditioned_score_model,
                     guide=score_parametrized_guide,
                     optim=pyro.optim.SGD({"lr": 0.001}),
                     loss=pyro.infer.Trace_ELBO())

losses = []
for t in range(1000):
    losses.append(svi.step(data))

{% endhighlight %}

Nice and simple, and I think it's a pretty nice abstraction to separate the generative model and the variational guide. The values of the `pyro.param` that have been optimized can be accessed through the output of the score_parametrized_guide function.

Here's a graph:

<div id= "footballGraph"></div>

Cool graph eh!

Here's a simulation

<div id="footballGameSim"></div>