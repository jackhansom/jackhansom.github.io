---
layout: post
title: "Probabilistic football model for the world cup"
description: "A ton of text to test readability."
tags: [data science, football, Variational Inference, Pyro]
categories: [data science]
jsarr:
- graphs/world_cup.js
- graphs/world_cup_simulation.js
---

Probabilistic programming is an exciting area of data science, but one in which there are still many different competing frameworks (even only within the python ecosystem). I've been meaning to play around with one of the newer frameworks from Uber, and based on pytorch: Pyro.
Since the football world cup is just around the corner, I thought it would be a good opportunity to use Pyro to not only try to predict the result of any of the matches, but also do something thta probabilistic programming is particularly useful for: inferring hidden variables.

Pyro's framework seems especially designed for inference through variational inference. Inference with Pyro requires mainly two things:
- A generative model, specified through a function
- A specification of the functional form of the posterior, which they call a 'guide'

This is quite a simple and elegant way of specifying how to run inference on what can be fairly complex models. 

<!-- more -->

## Simple model

Imagine for a second that you've never actually seen a football game, you have no idea how it is played and the only thing you know about it is that it is played (usually) over 90 minutes, involves two teams and scores typically range from 0 to 5. The simplest hypothesis you'd probably come up with is that each team has a given rate of goal scoring (e.g. team A scores 2.3 goals per game on average), and as a first approximation you can assume that the rate is constant and doesn't depend on the other team, the weather, the tournament etc. This is a very simple Poisson model, in which the only latent variable is the team's scoring rate! Here's an example of how to specify it in Pyro

{% highlight python %}

# data specification
data = [
    {
    "team A": 0,
    "team B": 1
},
]

def score_model(data):
    # separate prior over scoring rate for every team
    rate_dict = {team: pyro.sample("scoring_rate_{}".format(team), 
                dist.Gamma(conc_hyperprior, 1.0 / inv_rate_hyperprior))
                for team in teams}

    # now we can generate data for all the games in our dataset
    # the `obs=` kwarg signifies that we're also going to calculate
    # the likelihood under this model for a later optimisation
    for i, game in enumerate(data):
        for j, team in enumerate(sorted(game.keys())):
            score = torch.tensor(float(game[team]))
            rate = rate_dict[team]
            score_sample = dist.Poisson(rate)
            pyro.sample("score_game{}_{}".format(i, team), score_sample, obs=score)

# here we condition the generative model on the measurements
conditioned_score_model = pyro.condition(score_model, data=data)

{% endhighlight %}

What Pyro does under the hood (when inference is run) is take this generative model and calculate the likelihood of the data under the model. 
Pyro has a parameter store, which will keep track of the likelihoods of the different measurements (score_game* here) for different values of the latent variables. We have also specified here what our priors are for our latent variable, where we simply take Gamma distributions for each team. So far this is a nice pythonic way of specifying the generative model, but we are still missing the functional form of the posterior. In this simple model, it will mirror the generative model nicely. We have to remember that variational inference is an optimization over some scalar parameters so the simplest way to do this is through optimizing over gamma parameters for each team separately. Pyro will look for any 'pyro.param' that is specified in the guide and will run an optimization over these parameters to maximize the evidence lower bound (ELBO). In this guide you need to specify how these pyro.param influence parameters within the generative model. In our case, we will optimize over the concentrations and inverse rates of a set of gamma posteriors. 

{% highlight python %}

def score_parametrized_guide(data):
    # we can start the optimization at the hyperprior, specified in
    # the `score_model`
    conc = {team: pyro.param("concentration_{}".format(team), 
                             conc_hyperprior)
           for team in teams}
    inv_rate = {team: pyro.param("inv_rate_{}".format(team), 
                                 inv_rate_hyperprior)
           for team in teams}
    # This mirrors the `scoring_rate_` parameters in the generative model
    rate_dict = {team: pyro.sample("scoring_rate_{}".format(team), dist.Gamma(conc[team], 1.0 / inv_rate[team]))
                 for team in teams}
    return rate_dict

{% endhighlight %}  

To run the inference, we just specify what kind of inference we want (stochastic variational inference in our case), the loss we're interested in, and the optimization algorithm

{% highlight python %}

# remember to clear any lingering params
pyro.clear_param_store()
svi = pyro.infer.SVI(model=conditioned_score_model,
                     guide=score_parametrized_guide,
                     optim=pyro.optim.SGD({"lr": 0.001}),
                     loss=pyro.infer.Trace_ELBO())

losses = []
for t in range(1000):
    losses.append(svi.step(data))

{% endhighlight %}

Nice and simple, and I think it's a pretty nice abstraction to separate the generative model and the variational guide. The values of the 'pyro.param' that have been optimized can be accessed through the output of the score_parametrized_guide function.

## More complex model

Given our simple model, it's now quite simple to extend it to a more complex one. Now, instead of assuming that each team's score rate is independent of the other team, we can construct a simple attack-defence model. Each team's scoring rate will be positively affected by their own attack score and negatively affected by their opponent's defence score. It's also pretty simple to add team-independent factors, like a home effect, into the model in a very pythonic way. The generative model will be quite similar to the simple one above:

{% highlight python %}

def score_model(data):
    # prior over attack and defence abilities
    att_dict = {team: pyro.sample("attack_{}".format(team),
                dist.Normal(loc=0, scale=4))
                for team in teams}
    def_dict = {team: pyro.sample("defence_{}".format(team),
                dist.Normal(loc=0, scale=4))
                for team in teams}
    home_effect = pyro.sample("home_effect", dist.Normal(loc=0.,
                              scale=1.))

    for i, game in enumerate(data):
        for j, team in enumerate(game.keys()):
            score = torch.tensor(float(game[team]))

            # a team's scoring rate is modified by their own attack,
            # the opponent's defence and a possible home effect
            rate = torch.exp(torch.tensor(
                att_dict[team] - def_dict[game.keys()[(j + 1)%2]] + (j == 0) * home_effect
            ))
            score_sample = dist.Poisson(rate)
            pyro.sample("score_game{}_{}".format(i, team), score_sample, obs=score)

# condition the generative model on the measurement
conditioned_score_model = pyro.condition(score_model, data=data)

{% endhighlight %}

Now we can again posit the same posterior distribution as the prior (i.e. Normal), and take the means and variances as parameters to optimize over:

{% highlight python %}

def score_parametrized_guide(data):
    # we can start every team with a mean posterior attack and defence of 0.
    att_mean_param_dict = {team: pyro.param("attack_mean_{}".format(team), torch.tensor(att_mean_hyperprior))
                           for team in teams}
    att_std_param_dict = {team: pyro.param("attack_std_{}".format(team), torch.tensor(0.5), 
                                           constraint=constraints.positive)
                            for team in teams}
    att_dict = {team: pyro.sample("attack_{}".format(team), dist.Normal(loc=att_mean_param_dict[team], 
                                                                        scale=att_std_param_dict[team]
                                                                        ))
                for team in teams}

    def_mean_param_dict = {team: pyro.param("defence_mean_{}".format(team), torch.tensor(0.))
                for team in teams}
    def_std_param_dict = {team: pyro.param("defence_std_{}".format(team), torch.tensor(0.5),
                                          constraint=constraints.positive)
                for team in teams}
    def_dict = {team: pyro.sample("defence_{}".format(team), dist.Normal(loc=def_mean_param_dict[team], 
                                                                        scale=def_std_param_dict[team]))
                for team in teams}
    
    # home effect
    he_mean = pyro.param("home_effect_mean", torch.tensor(0.))
    he_std = pyro.param("home_effect_std", torch.tensor(.1), constraint=constraints.positive)
    he = pyro.sample("home_effect", dist.Normal(loc=he_mean, scale=he_std))
    return att_dict, def_dict, he

{% endhighlight %}

The inference step is exactly the same as the simple model, there are only a few more parameters to store per team. 
We can feed some historical data from international football results (https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017) and infer attack-defence values for each of the teams in this year's world cup. I split the data up in 4-year chunks and ran the same inference for every period, meaning we can see how each team has fluctuated in attack and defence. 

Take a look at this graph:

<div id= "footballGraph"></div>

In addition, because the generative model is pretty straightforward, we can run thousands of predictions per game, and get some predictions in the browser. The following table shows the win-draw-loss probabilities, as well as the most likely results according to the model

Here's a simulation

<div id="footballGameSim"></div>